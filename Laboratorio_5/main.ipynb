{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import utils\n",
    "import copy\n",
    "from naive_bayes import NaiveBayes\n",
    "import id3\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos \"Autism-Adult-Data.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = utils.read_file('Autism-Adult-Data.arff')\n",
    "data_set = examples[0]  # Datos\n",
    "metadata = examples[1]  # Metadatos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_attr = 'Class/ASD'\n",
    "attributes = ['A1_Score', 'A2_Score','A3_Score','A4_Score','A5_Score', 'A6_Score','A7_Score', 'A8_Score','A9_Score',\n",
    "              'A10_Score','age','gender','ethnicity','jundice','austim','contry_of_res','used_app_before','age_desc',\n",
    "              'relation']\n",
    "categorical_atts = ['A1_Score','A2_Score','A3_Score','A4_Score','A5_Score','A6_Score','A7_Score','A8_Score','A9_Score',\n",
    "                    'A10_Score','gender','ethnicity','jundice','austim','contry_of_res','used_app_before','age_desc',\n",
    "                    'relation']\n",
    "non_categorical_atts = ['age']\n",
    "weight = []\n",
    "categorical_atts_indexes = [0,1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18]\n",
    "non_categorical_atts_indexes = [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de datos\n",
    "A continuanción se procedere a transformar los datos. En primer lugar se busca desaparecer las instancias con valores faltante, para eso se remplaza los valores faltantes por el valor mas común en el atributo. En segundo lugar se procede a transformar los distintos tipos de valores a valores númericos, debido a que regresión logística opera con valores numéricos. Para realizar esto se aplica one hot encoding a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se usa most-common para manejar los missing values\n",
    "data = utils.process_missing_values(data_set, attributes, True)\n",
    "# Decode bytes\n",
    "data = utils.decode_data(data)\n",
    "\n",
    "# Extraemos el target attribute\n",
    "data_ext, data_target_attributes = utils.extract_target_attributes(data)\n",
    "\n",
    "# one hot encoding\n",
    "numeric_data = utils.one_hot_encoding(data_ext, categorical_atts, \n",
    "                                      categorical_atts_indexes, non_categorical_atts, \n",
    "                                      non_categorical_atts_indexes)\n",
    "numeric_attributes = list(numeric_data[0].keys())\n",
    "\n",
    "# Insertamos target attribute\n",
    "utils.insert_target_attributes(numeric_data, target_attr, data_target_attributes)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización\n",
    "Se normalizan los valores numéricos utilizando la técnica de min_max: se re escalan los valores a un rango entre 0 y 1\n",
    "![image.png](attachment:image.png)\n",
    "El re escalamiento se ejecuta en tiempo de entrenamiento, y el conjunto de validación se re escala utilizando los mismos parámetros (min y max) que se calcularon previamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Se divide el conjunto de datos\n",
    "numeric_validation_set, numeric_training_set = utils.split_20_80(numeric_data)\n",
    "\n",
    "\n",
    "# Normalizamos el training set utilizando la tecnica min-max\n",
    "training_set_scaled, scalation_parameters = utils.scale(copy.deepcopy(numeric_training_set), \n",
    "                                                        numeric_attributes,use_standarization)\n",
    "\n",
    "# Normalizamos el validation set utilizando la tecnica min-max y\n",
    "# los valores que usamos para normalizar el training\n",
    "validation_set_scaled = []\n",
    "for instance in numeric_validation_set:\n",
    "    scaled_instance = utils.scale_instance(copy.deepcopy(instance), scalation_parameters, use_standarization)\n",
    "    validation_set_scaled.append(scaled_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Búsqueda de mejores parametros iniciales\n",
    "Para aplicar regresión logística se requiere definir dos parámetros, uno es el weight y otro es el alpha.\n",
    "El vector de weight es el que se debe ajustar durante el algorimo de regresión logística y este es fundamental para clasificar las instancias. Partiendo de un buen weight inicial se acelera la convergencia. Con el vector weight se realiza un producto escalar con el vector transformado de las instancias, este resultado es aplicado a la funcion sigmoide para obtener la hipótesis de clasificación.\n",
    "Por el otro lado, el parámetro alpha se utiliza en el descenso por gradiente y representa el tamaño de intervalo en que una iteración de regresión logística se mueve en dirección del gradiente, con un alpha muy grande se corre el riesgo de no converger y con un alpha muy chico se puede tardar mucho en converger. Encontrar un alpha óptimo mejora el rendimiento del algoritmo.\n",
    "\n",
    "Para hayar los mejores valores se prueba el vector weight con los siguientes valores 10, 1, 0.1 y 0, y para alphas los valores 50, 10, 1 y 0.5. Alpha no puede ser \"0\" debido a que esto implica que el intervalo que avanza el algorimo es cero, lo que genera que no se actualizen los pesos weight.\n",
    "\n",
    "Para determinar cual es la mejor combinacion de valores se aplica regresión lógica a un conjunto de entrenamiento, la combinación que luego de 10 ajustes de weight logre el menor valor de la función costo es la que se usará en los siguientes bloques de código.\n",
    "\n",
    "Antes de realizar la busqueda de los parametros mencionados se debió normalizar los atributos de las instancias de lo contrario el algoritmo no funcionaría. Este comportamiento es debido a que si los atributos no estan normalizados, cuando se realiza el producto escalar entre el vector weight y la instancia, se da como resultado un número muy grande o muy chico, cuando se evalúa en la función sigmoide (para obtener ho(x)), dado a la representación de los números de una máquina se obtiene 0 o 1 en vez de un valor muy aproximado. Cuando se calcula la funcion Cost dependiendo del valor de atributo objetivo se realiza log(ho(x)) o log(1-ho(x)), lo que genera que se termine computando log(0) lo cual no existe. Al normalizar, el producto entre el vector weight y la instancia (wTx) no produce un resultado extremo y la funcion sigmoide no devuelve 0 o 1, pudiendo realizarse el algoritmo correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se realizan pruebas para obtiener los parametros con mejores resultados:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-78504de24eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# Se ajusta Weight con decenso por gradiente 10 veces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescentByGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLR_training_set_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLR_numeric_attributes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcostFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLR_training_set_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLR_numeric_attributes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ernes\\Desktop\\Aprendizaje automatico\\Laboratorios_MAA\\Laboratorio_5\\utils.py\u001b[0m in \u001b[0;36mdescentByGradient\u001b[1;34m(weight, data, a, attributesWithSesgo, target_attr)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m             \u001b[0mih0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculateH0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattributesWithSesgo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'YES'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                 \u001b[0msum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mih0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattributesWithSesgo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ernes\\Desktop\\Aprendizaje automatico\\Laboratorios_MAA\\Laboratorio_5\\utils.py\u001b[0m in \u001b[0;36mcalculateH0\u001b[1;34m(weight, instance, attributesWithSesgo)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculateH0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattributesWithSesgo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m     \u001b[0moTx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mscalarProductDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattributesWithSesgo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[0meExp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0moTx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[0mh0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meExp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ernes\\Desktop\\Aprendizaje automatico\\Laboratorios_MAA\\Laboratorio_5\\utils.py\u001b[0m in \u001b[0;36mscalarProductDict\u001b[1;34m(weight, instance, attributesWithSesgo)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributesWithSesgo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweightIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[0mweightIndex\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bestChoose = []\n",
    "posiblesWeight = [10, 1, 0.1, 0]\n",
    "posiblesAlpha = [50, 10, 1, 0.5]\n",
    "use_standarization = False\n",
    "\n",
    "\n",
    "# Se agrega 1 al inicio de los datos para\n",
    "# que el producto escalar tenga termino sesgo\n",
    "LR_numeric_attributes = copy.deepcopy(numeric_attributes)\n",
    "LR_numeric_attributes.insert(0,'sesgo')\n",
    "LR_training_set_scaled = copy.deepcopy(training_set_scaled)\n",
    "utils.insert_sesgo_one(LR_training_set_scaled)\n",
    "\n",
    "print(\"Se realizan pruebas para obtiener los parametros con mejores resultados:\")\n",
    "tablaParametros = []\n",
    "tablaParametros.append([\"Weight\", \"Alpha\", \"Costo obtenido\"])\n",
    "\n",
    "# Se inicializa el costo mínimo como inf (máximo float)\n",
    "minCost = float('inf')\n",
    "\n",
    "for w in range(4):\n",
    "    for a in range(4):\n",
    "        weight= []\n",
    "        \n",
    "        # Array de Weights para LR\n",
    "        for i in range(len(LR_numeric_attributes)):\n",
    "            weight += [posiblesWeight[w]]\n",
    "\n",
    "        # Constante alpha de LR\n",
    "        alpha = posiblesAlpha[a]\n",
    "\n",
    "        for i in range(10):\n",
    "            # Se ajusta Weight con decenso por gradiente 10 veces\n",
    "            weight = utils.descentByGradient(weight, LR_training_set_scaled, alpha, LR_numeric_attributes, target_attr)\n",
    "\n",
    "        cost = utils.costFunction(weight, LR_training_set_scaled, LR_numeric_attributes, target_attr)\n",
    "        tablaParametros.append([posiblesWeight[w], alpha, cost])\n",
    "\n",
    "        if (minCost > cost):\n",
    "            minCost = cost\n",
    "            bestChoose = [posiblesWeight[w], posiblesAlpha[a], cost]\n",
    "\n",
    "print(tabulate(tablaParametros, headers='firstrow', tablefmt='fancy_grid', stralign='right', showindex=True))\n",
    "print(\"El menor costo conseguido fue: \", bestChoose[2], \" con Weight = \", bestChoose[0], \" y alpha = \", bestChoose[1])\n",
    "print(\"A continuación se utilizarán los parametros obtenidos.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regresión lógica y comparación de resultado\n",
    "A continuacion se realizan 10 iteraciones de los algoritmos regresión lógica, KNN, Naive Bayes y ID3 entrenando con el 80% del conjunto de datos y validando con el 20% restante.\n",
    "En el caso de regresión lógica se utilizan los parametros encontrados en el segmento de codigo anterior.\n",
    "Finalmente se presenta la comparativa de promedio de errores de los cuatro algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_LR_total = 0\n",
    "errors_KNN_total = 0\n",
    "errors_NB_total = 0\n",
    "errors_ID3_total = 0\n",
    "\n",
    "tablaIteraciones = []\n",
    "tablaErroresIteraciones = []\n",
    "\n",
    "tablaIteraciones.append(['','',\"Iter 1\",\"Iter 2\",\"Iter 3\",\"Iter 4\",\"Iter 5\",\"Iter 6\",\"Iter 7\",\"Iter 8\"])\n",
    "tablaErroresIteraciones.append(['','',\"Iter 1\",\"Iter 2\",\"Iter 3\",\"Iter 4\",\"Iter 5\",\"Iter 6\",\"Iter 7\",\"Iter 8\",\"Promedio\"])\n",
    "\n",
    "for iter in range(8):\n",
    "    print('Realizando iteración ',iter +1, ':')\n",
    "    #######################################################################################\n",
    "    #######################       Regresión Logica           ##############################\n",
    "    #######################################################################################\n",
    "    \n",
    "    # Se divide el conjunto de datos\n",
    "    numeric_validation_set, numeric_training_set = utils.split_20_80(numeric_data)\n",
    "\n",
    "\n",
    "    use_standarization = False\n",
    "    # Normalizamos el training set utilizando la tecnica min-max\n",
    "    training_set_scaled, scalation_parameters = utils.scale(copy.deepcopy(numeric_training_set), numeric_attributes,use_standarization)\n",
    "\n",
    "\n",
    "    # Normalizamos el validation set utilizando la tecnica min-max y\n",
    "    # los valores que usamos para normalizar el training\n",
    "    validation_set_scaled = []\n",
    "    for instance in numeric_validation_set:\n",
    "        scaled_instance = utils.scale_instance(copy.deepcopy(instance), scalation_parameters, use_standarization)\n",
    "        validation_set_scaled.append(scaled_instance)\n",
    "\n",
    "\n",
    "    # Se agrega 1 al inicio de los datos para\n",
    "    # que el producto escalar tenga termino sesgo\n",
    "    LR_numeric_attributes = copy.deepcopy(numeric_attributes)\n",
    "    LR_numeric_attributes.insert(0,'sesgo')\n",
    "    LR_training_set_scaled = copy.deepcopy(training_set_scaled)\n",
    "    LR_validation_set_scaled = copy.deepcopy(validation_set_scaled)\n",
    "    utils.insert_sesgo_one(LR_training_set_scaled)\n",
    "    utils.insert_sesgo_one(LR_validation_set_scaled)\n",
    "\n",
    "    # Weights para LR\n",
    "    weight = []\n",
    "    for i in range(len(LR_numeric_attributes)):\n",
    "        weight += [bestChoose[0]]\n",
    "\n",
    "    # Constante alpha de LR\n",
    "    alpha = bestChoose[1]\n",
    "\n",
    "    # Costo anterior\n",
    "    cost = float('inf')\n",
    "\n",
    "    # La condición de parado son 15 iteraciones o una \n",
    "    # diferencia de costos menor a 0.0001\n",
    "    for i in range(15):\n",
    "        newCost = utils.costFunction(weight, LR_training_set_scaled, LR_numeric_attributes, target_attr)\n",
    "        dif =abs(cost - newCost)\n",
    "        \n",
    "        if (iter == 0):\n",
    "            row = 'Ajuste ' + repr(i)\n",
    "            tablaIteraciones.append([row])\n",
    "\n",
    "        tablaIteraciones[i+1].append(newCost)\n",
    "        \n",
    "        if (abs(cost - newCost) < 0.0001):\n",
    "            break\n",
    "        cost = newCost\n",
    "        weight = utils.descentByGradient(weight, LR_training_set_scaled, alpha, LR_numeric_attributes, target_attr)\n",
    "    \n",
    "    \n",
    "    # LR holdout validation\n",
    "    errors_LR = utils.LR_holdout_validation(LR_validation_set_scaled, target_attr, weight, LR_numeric_attributes)\n",
    "    errors_LR_total += errors_LR\n",
    "\n",
    "\n",
    "    #######################################################################################\n",
    "    #######################               KNN                ##############################\n",
    "    #######################################################################################\n",
    "    \n",
    "    # KNN holdout validation con k = 3 y usando pesos\n",
    "    errors_KNN = utils.KNN_holdout_validation(copy.deepcopy(training_set_scaled), copy.deepcopy(validation_set_scaled), target_attr, numeric_attributes, 3, True)\n",
    "    errors_KNN_total += errors_KNN[1]\n",
    "\n",
    "    \n",
    "    #######################################################################################\n",
    "    #######################         Naive Bayes              ##############################\n",
    "    #######################################################################################\n",
    "    \n",
    "    nb_classifier = NaiveBayes(copy.deepcopy(training_set_scaled), numeric_attributes, target_attr)\n",
    "    errors_NB = nb_classifier.holdout_validation(copy.deepcopy(validation_set_scaled), target_attr)\n",
    "    errors_NB_total += errors_NB[1]\n",
    "\n",
    "    \n",
    "    #######################################################################################\n",
    "    #######################            ID3                   ##############################\n",
    "    #######################################################################################\n",
    "    \n",
    "    tree = id3.ID3_algorithm(training_set_scaled, numeric_attributes, target_attr, True, False)\n",
    "    errors_ID3 = id3.validation(tree, validation_set_scaled, target_attr)\n",
    "    errors_ID3_total += errors_ID3\n",
    "\n",
    "    if (iter == 0):\n",
    "        tablaErroresIteraciones.append(['Regresión lógica'])\n",
    "        tablaErroresIteraciones.append(['KNN'])\n",
    "        tablaErroresIteraciones.append(['Naive Bayes'])\n",
    "        tablaErroresIteraciones.append(['ID3'])\n",
    "\n",
    "    tablaErroresIteraciones[1].append(errors_LR)\n",
    "    tablaErroresIteraciones[2].append(errors_KNN[1])\n",
    "    tablaErroresIteraciones[3].append(errors_NB[1])\n",
    "    tablaErroresIteraciones[4].append(errors_ID3)\n",
    "\n",
    "tablaErroresIteraciones[1].append(errors_LR_total/8)\n",
    "tablaErroresIteraciones[2].append(errors_KNN_total/8)\n",
    "tablaErroresIteraciones[3].append(errors_NB_total/8)\n",
    "tablaErroresIteraciones[4].append(errors_ID3_total/8)\n",
    "\n",
    "print(tabulate(tablaIteraciones, headers='firstrow', tablefmt='fancy_grid', stralign='right', showindex=True))\n",
    "print()\n",
    "print(tabulate(tablaErroresIteraciones, headers='firstrow', tablefmt='fancy_grid', stralign='right', showindex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
