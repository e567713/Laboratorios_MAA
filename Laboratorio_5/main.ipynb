{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import utils\n",
    "import copy\n",
    "from naive_bayes import NaiveBayes\n",
    "import id3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos \"Autism-Adult-Data.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = utils.read_file('Autism-Adult-Data.arff')\n",
    "data_set = examples[0]  # Datos\n",
    "metadata = examples[1]  # Metadatos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_attr = 'Class/ASD'\n",
    "attributes = ['A1_Score', 'A2_Score','A3_Score','A4_Score','A5_Score', 'A6_Score','A7_Score', 'A8_Score','A9_Score',\n",
    "              'A10_Score','age','gender','ethnicity','jundice','austim','contry_of_res','used_app_before','age_desc',\n",
    "              'relation']\n",
    "categorical_atts = ['A1_Score','A2_Score','A3_Score','A4_Score','A5_Score','A6_Score','A7_Score','A8_Score','A9_Score',\n",
    "                    'A10_Score','gender','ethnicity','jundice','austim','contry_of_res','used_app_before','age_desc',\n",
    "                    'relation']\n",
    "non_categorical_atts = ['age']\n",
    "weight = []\n",
    "categorical_atts_indexes = [0,1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18]\n",
    "non_categorical_atts_indexes = [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de datos\n",
    "A continuancion se procederá a procesar los valores faltantes y luego a transformar los atributos a atributos númericos, insertando los valores mas comunes en los atributos faltantes y realizando one hot encoding para obtener atributos númericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se usa most-common para manejar los missing values\n",
    "data = utils.process_missing_values(data_set, attributes, True)\n",
    "# Decode bytes\n",
    "data = utils.decode_data(data)\n",
    "\n",
    "# Sacamos el target attribute\n",
    "data_ext, data_target_attributes = utils.extract_target_attributes(data)\n",
    "\n",
    "# one hot encoding\n",
    "numeric_data = utils.one_hot_encoding(data_ext, categorical_atts, categorical_atts_indexes, non_categorical_atts, non_categorical_atts_indexes)\n",
    "numeric_attributes = list(numeric_data[0].keys())\n",
    "\n",
    "# insertamos target attribute\n",
    "utils.insert_target_attributes(numeric_data, target_attr, data_target_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Busqueda de mejores parametros iniciales\n",
    "Se busca el mejor weight y alfa inicial.\n",
    "Para hayar los mejores valores se prueban las combinaciones de weight (10, 1, 0.1 y 0), y de alphas (50, 10, 1 y 0.5).\n",
    "Para determinar cual es la mejor combinacion de valores se aplica regresión lógica a un conjunto de entrenamiento, la combinación que luego de 10 ajustes de weight logre el menor valor de la función costo es la indicada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El menor costo conseguido fue:  0.0127665186020805  con Weight =  0.1  y alpha =  10\n"
     ]
    }
   ],
   "source": [
    "#  Se divide el conjunto de datos\n",
    "numeric_validation_set, numeric_training_set = utils.split_20_80(numeric_data)\n",
    "\n",
    "bestChoose = []\n",
    "posiblesWeight = [10, 1, 0.1, 0]\n",
    "posiblesAlpha = [50, 10, 1, 0.5]\n",
    "use_standarization = False\n",
    "\n",
    "\n",
    "# Normalizamos el training set utilizando la tecnica min-max\n",
    "training_set_scaled, scalation_parameters = utils.scale(copy.deepcopy(numeric_training_set), numeric_attributes,use_standarization)\n",
    "\n",
    "# Normalizamos el validation set utilizando la tecnica min-max y\n",
    "# los valores que usamos para normalizar el training\n",
    "validation_set_scaled = []\n",
    "for instance in numeric_validation_set:\n",
    "    scaled_instance = utils.scale_instance(copy.deepcopy(instance), scalation_parameters, use_standarization)\n",
    "    validation_set_scaled.append(scaled_instance)\n",
    "\n",
    "# Se agrega 1 al inicio de los datos para\n",
    "# que el producto escalar tenga termino sesgo\n",
    "LR_numeric_attributes = copy.deepcopy(numeric_attributes)\n",
    "LR_numeric_attributes.insert(0,'sesgo')\n",
    "LR_training_set_scaled = copy.deepcopy(training_set_scaled)\n",
    "utils.insert_sesgo_one(LR_training_set_scaled)\n",
    "\n",
    "# Se inicializa el costo mínimo como inf (máximo float)\n",
    "minCost = float('inf')\n",
    "\n",
    "for w in range(4):\n",
    "    for a in range(4):\n",
    "        weight= []\n",
    "        \n",
    "        # Array de Weights para LR\n",
    "        for i in range(len(LR_numeric_attributes)):\n",
    "            weight += [posiblesWeight[w]]\n",
    "\n",
    "        # Constante alpha de LR\n",
    "        alpha = posiblesAlpha[a]\n",
    "\n",
    "        for i in range(10):\n",
    "            # Se ajusta Weight con decenso por gradiente 10 veces\n",
    "            weight = utils.descentByGradient(weight, LR_training_set_scaled, alpha, LR_numeric_attributes, target_attr)\n",
    "\n",
    "        cost = utils.costFunction(weight, LR_training_set_scaled, LR_numeric_attributes, target_attr)\n",
    "\n",
    "        if (minCost > cost):\n",
    "            minCost = cost\n",
    "            bestChoose = [posiblesWeight[w], posiblesAlpha[a], cost]\n",
    "\n",
    "print(\"El menor costo conseguido fue: \", bestChoose[2], \" con Weight = \", bestChoose[0], \" y alpha = \", bestChoose[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regresión lógica y comparación de resultado\n",
    "A continuacion se realizan 10 iteraciones de los algoritmos regresión lógica, KNN, Naive Bayes y ID3 entrenando con el 80% del conjunto de datos y validando con el 20% restante.\n",
    "En el caso de regresión lógica se utilizan los parametros encontrados en el segmento de codigo anterior.\n",
    "Finalmente se presenta la comparativa de promedio de errores de los cuatro algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors_LR_total = 0\n",
    "errors_KNN_total = 0\n",
    "errors_NB_total = 0\n",
    "errors_ID3_total = 0\n",
    "\n",
    "for iter in range(10):\n",
    "    # Se divide el conjunto de datos\n",
    "    numeric_validation_set, numeric_training_set = utils.split_20_80(numeric_data)\n",
    "\n",
    "\n",
    "    use_standarization = False\n",
    "    # Normalizamos el training set utilizando la tecnica min-max\n",
    "    training_set_scaled, scalation_parameters = utils.scale(copy.deepcopy(numeric_training_set), numeric_attributes,use_standarization)\n",
    "\n",
    "\n",
    "    # Normalizamos el validation set utilizando la tecnica min-max y\n",
    "    # los valores que usamos para normalizar el training\n",
    "    validation_set_scaled = []\n",
    "    for instance in numeric_validation_set:\n",
    "        scaled_instance = utils.scale_instance(copy.deepcopy(instance), scalation_parameters, use_standarization)\n",
    "        validation_set_scaled.append(scaled_instance)\n",
    "\n",
    "\n",
    "    # Sesgo para LR\n",
    "    LR_numeric_attributes = copy.deepcopy(numeric_attributes)\n",
    "    LR_numeric_attributes.insert(0,'sesgo')\n",
    "    LR_training_set_scaled = copy.deepcopy(training_set_scaled)\n",
    "    LR_validation_set_scaled = copy.deepcopy(validation_set_scaled)\n",
    "    utils.insert_sesgo_one(LR_training_set_scaled)\n",
    "    utils.insert_sesgo_one(LR_validation_set_scaled)\n",
    "\n",
    "    # Weights para LR\n",
    "    weight = []\n",
    "    for i in range(len(LR_numeric_attributes)):\n",
    "        weight += [bestChose[0]]\n",
    "\n",
    "    # Constante alpha de LR\n",
    "    alpha = bestChose[1]\n",
    "\n",
    "    # Costo anterior\n",
    "    cost = sys.maxsize\n",
    "\n",
    "    for i in range(100):\n",
    "        newCost = utils.costFunction(weight, LR_training_set_scaled, LR_numeric_attributes, target_attr)\n",
    "        dif =abs(cost - newCost)\n",
    "        if (abs(cost - newCost) < 0.0001):\n",
    "            break\n",
    "        print(\"Costo\", i + 1)\n",
    "        print(newCost)\n",
    "        print()\n",
    "        cost = newCost\n",
    "        weight = utils.descentByGradient(weight, LR_training_set_scaled, alpha, LR_numeric_attributes, target_attr)\n",
    "\n",
    "\n",
    "    # LR holdout validation\n",
    "    errors_LR = utils.LR_holdout_validation(LR_validation_set_scaled, target_attr, weight, LR_numeric_attributes)\n",
    "    # print(\"Cantidad de errores registrados LR:\", errors_LR)\n",
    "    errors_LR_total += errors_LR\n",
    "\n",
    "\n",
    "    # KNN holdout validation con k = 3 y usando pesos\n",
    "    errors_KNN = utils.KNN_holdout_validation(copy.deepcopy(training_set_scaled), copy.deepcopy(validation_set_scaled), target_attr, numeric_attributes, 3, True)\n",
    "    # print('cantidad de errores KNN:',errors_KNN[1])\n",
    "    errors_KNN_total += errors_KNN[1]\n",
    "\n",
    "    # Para naive bayes hay que ver si pasarle el scaled o no,\n",
    "    # se rompe con el scaled, habría que usar la normalizacion de naive bayes?\n",
    "\n",
    "    nb_classifier = NaiveBayes(copy.deepcopy(training_set_scaled), numeric_attributes, target_attr)\n",
    "    # print(nb_classifier.attributes_values)\n",
    "    errors_NB = nb_classifier.holdout_validation(copy.deepcopy(validation_set_scaled), target_attr)\n",
    "    # print('cantidad de errores NB:',errors_NB[1])\n",
    "    errors_NB_total += errors_NB[1]\n",
    "\n",
    "    tree = id3.ID3_algorithm(training_set_scaled, numeric_attributes, target_attr, True, False)\n",
    "    errors_ID3 = id3.validation(tree, validation_set_scaled, target_attr)\n",
    "    # print(\"cantidad de errores ID3:\",errors_ID3)\n",
    "    errors_ID3_total += errors_ID3\n",
    "\n",
    "print(\"Promedio de errores:\")\n",
    "print(\"LR: \", errors_LR_total/10)\n",
    "print(\"KNN: \", errors_KNN_total/10)\n",
    "print(\"NB: \", errors_NB_total/10)\n",
    "print(\"ID3: \", errors_ID3_total/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
