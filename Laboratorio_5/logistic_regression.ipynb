{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import utils\n",
    "import copy\n",
    "from naive_bayes import NaiveBayes\n",
    "import id3\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos \"Autism-Adult-Data.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = utils.read_file('Autism-Adult-Data.arff')\n",
    "data_set = examples[0]  # Datos\n",
    "metadata = examples[1]  # Metadatos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_attr = 'Class/ASD'\n",
    "attributes = ['A1_Score', 'A2_Score','A3_Score','A4_Score','A5_Score', 'A6_Score','A7_Score', 'A8_Score','A9_Score',\n",
    "              'A10_Score','age','gender','ethnicity','jundice','austim','contry_of_res','used_app_before','age_desc',\n",
    "              'relation']\n",
    "categorical_atts = ['A1_Score','A2_Score','A3_Score','A4_Score','A5_Score','A6_Score','A7_Score','A8_Score','A9_Score',\n",
    "                    'A10_Score','gender','ethnicity','jundice','austim','contry_of_res','used_app_before','age_desc',\n",
    "                    'relation']\n",
    "non_categorical_atts = ['age']\n",
    "weight = []\n",
    "categorical_atts_indexes = [0,1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18]\n",
    "non_categorical_atts_indexes = [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de datos\n",
    "A continuanción se procedere a transformar los datos. En primer lugar se busca desaparecer las instancias con valores faltante, para eso se remplaza los valores faltantes por el valor mas común en el atributo. En segundo lugar se procede a transformar los distintos tipos de valores a valores númericos, debido a que regresión logística opera con valores numéricos. Para realizar esto se aplica one hot encoding a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se usa most-common para manejar los missing values\n",
    "data = utils.process_missing_values(data_set, attributes, True)\n",
    "# Decode bytes\n",
    "data = utils.decode_data(data)\n",
    "\n",
    "# Extraemos el target attribute\n",
    "data_ext, data_target_attributes = utils.extract_target_attributes(data)\n",
    "\n",
    "# one hot encoding\n",
    "numeric_data = utils.one_hot_encoding(data_ext, categorical_atts, \n",
    "                                      categorical_atts_indexes, non_categorical_atts, \n",
    "                                      non_categorical_atts_indexes)\n",
    "numeric_attributes = list(numeric_data[0].keys())\n",
    "\n",
    "# Insertamos target attribute\n",
    "utils.insert_target_attributes(numeric_data, target_attr, data_target_attributes)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización\n",
    "Se normalizan los valores numéricos utilizando la técnica de min_max: se re escalan los valores a un rango entre 0 y 1\n",
    "![image.png](attachment:image.png)\n",
    "El re escalamiento se ejecuta en tiempo de entrenamiento, y el conjunto de validación se re escala utilizando los mismos parámetros (min y max) que se calcularon previamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_standarization = False\n",
    "\n",
    "#  Se divide el conjunto de datos\n",
    "numeric_validation_set, numeric_training_set = utils.split_20_80(numeric_data)\n",
    "\n",
    "\n",
    "# Normalizamos el training set utilizando la tecnica min-max\n",
    "training_set_scaled, scalation_parameters = utils.scale(copy.deepcopy(numeric_training_set), \n",
    "                                                        numeric_attributes,use_standarization)\n",
    "\n",
    "# Normalizamos el validation set utilizando la tecnica min-max y\n",
    "# los valores que usamos para normalizar el training\n",
    "validation_set_scaled = []\n",
    "for instance in numeric_validation_set:\n",
    "    scaled_instance = utils.scale_instance(copy.deepcopy(instance), scalation_parameters, use_standarization)\n",
    "    validation_set_scaled.append(scaled_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Búsqueda de mejores parametros iniciales\n",
    "Para aplicar regresión logística se requiere definir dos parámetros, uno es el weight y otro es el alpha.\n",
    "El vector de weight es el que se debe ajustar durante el algorimo de regresión logística y este es fundamental para clasificar las instancias. Partiendo de un buen weight inicial se acelera la convergencia. Con el vector weight se realiza un producto escalar con el vector transformado de las instancias, este resultado es aplicado a la funcion sigmoide para obtener la hipótesis de clasificación.\n",
    "Por el otro lado, el parámetro alpha se utiliza en el descenso por gradiente y representa el tamaño de intervalo en que una iteración de regresión logística se mueve en dirección del gradiente, con un alpha muy grande se corre el riesgo de no converger y con un alpha muy chico se puede tardar mucho en converger. Encontrar un alpha óptimo mejora el rendimiento del algoritmo.\n",
    "\n",
    "Para hayar los mejores valores se prueba el vector weight con los siguientes valores 10, 1, 0.1 y 0, y para alphas los valores 50, 10, 1 y 0.5. Alpha no puede ser \"0\" debido a que esto implica que el intervalo que avanza el algorimo es cero, lo que genera que no se actualizen los pesos weight.\n",
    "\n",
    "Para determinar cual es la mejor combinacion de valores se aplica regresión lógica a un conjunto de entrenamiento, la combinación que luego de 10 ajustes de weight logre el menor valor de la función costo es la que se usará en los siguientes bloques de código.\n",
    "\n",
    "Antes de realizar la busqueda de los parametros mencionados se debió normalizar los atributos de las instancias de lo contrario el algoritmo no funcionaría. Este comportamiento es debido a que si los atributos no estan normalizados, cuando se realiza el producto escalar entre el vector weight y la instancia, se da como resultado un número muy grande o muy chico, cuando se evalúa en la función sigmoide (para obtener ho(x)), dado a la representación de los números de una máquina se obtiene 0 o 1 en vez de un valor muy aproximado. Cuando se calcula la funcion Cost dependiendo del valor de atributo objetivo se realiza log(ho(x)) o log(1-ho(x)), lo que genera que se termine computando log(0) lo cual no existe. Al normalizar, el producto entre el vector weight y la instancia (wTx) no produce un resultado extremo y la funcion sigmoide no devuelve 0 o 1, pudiendo realizarse el algoritmo correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se realizan pruebas para obtener los parámetros iniciales con mejores resultados:\n",
      "╒════╤══════════╤═════════╤══════════════════╕\n",
      "│    │   Weight │   Alpha │   Costo obtenido │\n",
      "╞════╪══════════╪═════════╪══════════════════╡\n",
      "│  0 │     10   │    50   │        0.07144   │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│  1 │     10   │    10   │        0.266409  │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│  2 │     10   │     1   │      inf         │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│  3 │     10   │     0.5 │      inf         │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│  4 │      1   │    50   │        0.0206753 │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│  5 │      1   │    10   │        0.0187287 │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│  6 │      1   │     1   │        0.0832554 │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│  7 │      1   │     0.5 │        0.115361  │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│  8 │      0.1 │    50   │        0.0201981 │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│  9 │      0.1 │    10   │        0.0130426 │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│ 10 │      0.1 │     1   │        0.0699795 │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│ 11 │      0.1 │     0.5 │        0.10058   │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│ 12 │      0   │    50   │        0.0322926 │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│ 13 │      0   │    10   │        0.0147113 │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│ 14 │      0   │     1   │        0.0698396 │\n",
      "├────┼──────────┼─────────┼──────────────────┤\n",
      "│ 15 │      0   │     0.5 │        0.100244  │\n",
      "╘════╧══════════╧═════════╧══════════════════╛\n",
      "El menor costo conseguido fue:  0.013042583343210285  con Weight =  0.1  y alpha =  10\n",
      "A continuación se utilizarán los parametros obtenidos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestChoose = []\n",
    "posiblesWeight = [10, 1, 0.1, 0]\n",
    "posiblesAlpha = [50, 10, 1, 0.5]\n",
    "\n",
    "# Se agrega 1 al inicio de los datos para\n",
    "# que el producto escalar tenga termino sesgo\n",
    "LR_numeric_attributes = copy.deepcopy(numeric_attributes)\n",
    "LR_numeric_attributes.insert(0,'sesgo')\n",
    "LR_training_set_scaled = copy.deepcopy(training_set_scaled)\n",
    "utils.insert_sesgo_one(LR_training_set_scaled)\n",
    "\n",
    "print(\"Se realizan pruebas para obtener los parámetros iniciales con mejores resultados:\")\n",
    "tablaParametros = []\n",
    "tablaParametros.append([\"Weight\", \"Alpha\", \"Costo obtenido\"])\n",
    "\n",
    "# Se inicializa el costo mínimo como inf (máximo float)\n",
    "minCost = float('inf')\n",
    "\n",
    "for w in range(4):\n",
    "    for a in range(4):\n",
    "        weight= []\n",
    "        \n",
    "        # Array de Weights para LR\n",
    "        for i in range(len(LR_numeric_attributes)):\n",
    "            weight += [posiblesWeight[w]]\n",
    "\n",
    "        # Constante alpha de LR\n",
    "        alpha = posiblesAlpha[a]\n",
    "\n",
    "        for i in range(10):\n",
    "            # Se ajusta Weight con decenso por gradiente 10 veces\n",
    "            weight = utils.descentByGradient(weight, LR_training_set_scaled, alpha, LR_numeric_attributes, target_attr)\n",
    "\n",
    "        cost = utils.costFunction(weight, LR_training_set_scaled, LR_numeric_attributes, target_attr)\n",
    "        tablaParametros.append([posiblesWeight[w], alpha, cost])\n",
    "\n",
    "        if (minCost > cost):\n",
    "            minCost = cost\n",
    "            bestChoose = [posiblesWeight[w], posiblesAlpha[a], cost]\n",
    "\n",
    "print(tabulate(tablaParametros, headers='firstrow', tablefmt='fancy_grid', stralign='right', showindex=True))\n",
    "print(\"El menor costo conseguido fue: \", bestChoose[2], \" con Weight = \", bestChoose[0], \" y alpha = \", bestChoose[1])\n",
    "print(\"A continuación se utilizarán los parametros obtenidos.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regresión logística y comparación de resultado\n",
    "Los algoritmos a comparar son: regresión logística, KNN, Naive Bayes e ID3<br>\n",
    "Para la obtención de los resultados se utiliza el 80% del conjunto de datos como conjunto de entrenamiento y se valida con el 20% restante. Se ejecuta *'total_iters'* (8 en ) veces cada algoritmo variando el conjunto seleccionado de datos de entrenamiento y validación, promediando la suma de errores de cada iteración.<br>\n",
    "En el caso de **regresión logística** se utilizan los parámetros encontrados en el segmento de código anterior.<br>\n",
    "**KNN** se ejecuta comprando los 3 valores más cercanos (k = 3) y utilizando pesos (los ejemplos de entrenamiento más cercanos a mi instancia a evaluar son más influyentes).<br>\n",
    "**Naive Bayes** utiliza distribución normal para el cálculo de la probabilidad condicional para los atributos numéricos.<br>\n",
    "**ID3** particiona en dos subconjuntos a partir de un cierto límite (threshold) los atributos numéricos<br>\n",
    "\n",
    "Finalmente se presenta la comparativa de promedio de errores de los cuatro algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando iteración  1 :\n",
      "Realizando iteración  2 :\n",
      "Realizando iteración  3 :\n",
      "Realizando iteración  4 :\n",
      "Realizando iteración  5 :\n",
      "Realizando iteración  6 :\n",
      "Realizando iteración  7 :\n",
      "Realizando iteración  8 :\n",
      "\n",
      "El tamaño del conjunto de validación es: 140\n",
      "\n",
      "\n",
      "La siguiente tabla muestra el valor retornado por la función de costo\n",
      "en cada uno de los ajustes (filas), para cada iteración (columnas)\n",
      "\n",
      "╒════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╤════════════╕\n",
      "│        │     Iter 1 │     Iter 2 │     Iter 3 │     Iter 4 │     Iter 5 │     Iter 6 │     Iter 7 │     Iter 8 │\n",
      "╞════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╡\n",
      "│  Aj. 0 │ 0.648898   │ 0.6365     │ 0.654568   │ 0.662917   │ 0.639048   │ 0.64183    │ 0.641914   │ 0.647421   │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│  Aj. 1 │ 6.75299    │ 6.9106     │ 6.6159     │ 6.51472    │ 6.92708    │ 6.82093    │ 6.84872    │ 6.75532    │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│  Aj. 2 │ 2.71805    │ 2.3873     │ 2.8208     │ 3.00336    │ 2.47749    │ 2.52827    │ 2.50583    │ 2.69026    │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│  Aj. 3 │ 0.0438146  │ 0.161477   │ 0.0500493  │ 0.100626   │ 0.123285   │ 0.0780615  │ 0.0897163  │ 0.0417734  │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│  Aj. 4 │ 0.0345836  │ 0.0849298  │ 0.0350519  │ 0.0507207  │ 0.0588625  │ 0.0403931  │ 0.0462772  │ 0.0330946  │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│  Aj. 5 │ 0.0277294  │ 0.0439902  │ 0.0285371  │ 0.0295781  │ 0.0330352  │ 0.0269867  │ 0.0297244  │ 0.0270781  │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│  Aj. 6 │ 0.0224177  │ 0.0284761  │ 0.0235829  │ 0.0222689  │ 0.0243113  │ 0.020427   │ 0.0230454  │ 0.022685   │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│  Aj. 7 │ 0.0182646  │ 0.0206461  │ 0.0195288  │ 0.0178871  │ 0.0193269  │ 0.0163914  │ 0.0188681  │ 0.019201   │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│  Aj. 8 │ 0.015092   │ 0.0162066  │ 0.0162718  │ 0.0150027  │ 0.0158878  │ 0.0136233  │ 0.0159554  │ 0.0163841  │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│  Aj. 9 │ 0.012721   │ 0.0133257  │ 0.0137275  │ 0.0128629  │ 0.0133119  │ 0.011644   │ 0.0137214  │ 0.0140987  │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│ Aj. 10 │ 0.0109697  │ 0.0113317  │ 0.0117763  │ 0.0112314  │ 0.0113516  │ 0.0101905  │ 0.011968   │ 0.0122568  │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│ Aj. 11 │ 0.00967152 │ 0.0099064  │ 0.010289   │ 0.00996623 │ 0.00985813 │ 0.00909914 │ 0.0105788  │ 0.0107855  │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│ Aj. 12 │ 0.00869529 │ 0.00884814 │ 0.0091503  │ 0.00897326 │ 0.00871794 │ 0.0082604  │ 0.00947298 │ 0.00961771 │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│ Aj. 13 │ 0.00794662 │ 0.00803742 │ 0.00826881 │ 0.00818385 │ 0.00784131 │ 0.00760225 │ 0.00858765 │ 0.00869148 │\n",
      "├────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────────┤\n",
      "│ Aj. 14 │ 0.00736015 │ 0.00739866 │ 0.00757654 │ 0.00754803 │ 0.0071598  │ 0.00707576 │ 0.00787373 │ 0.00795368 │\n",
      "╘════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╧════════════╛\n",
      "\n",
      "En la última tabla se observa el porcentaje de errores retornado por cada algoritmo (filas)\n",
      "en cada iteración (columnas), como también el porcentaje del promedio de errrores\n",
      "de todas las iteraciones (última columna)\n",
      "\n",
      "╒═════════════════╤══════════╤══════════╤══════════╤══════════╤══════════╤══════════╤══════════╤══════════╤════════════╕\n",
      "│                 │   Iter 1 │   Iter 2 │   Iter 3 │   Iter 4 │   Iter 5 │   Iter 6 │   Iter 7 │   Iter 8 │   Promedio │\n",
      "╞═════════════════╪══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪══════════╪════════════╡\n",
      "│ Regr. logística │    0.0 % │   1.43 % │    0.0 % │    0.0 % │    0.0 % │   2.14 % │   1.43 % │    0.0 % │     0.62 % │\n",
      "├─────────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┤\n",
      "│             KNN │   2.14 % │   3.57 % │   3.57 % │   6.43 % │    5.0 % │   2.86 % │   4.29 % │    5.0 % │     4.11 % │\n",
      "├─────────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┤\n",
      "│     Naive Bayes │    5.0 % │   7.14 % │  12.14 % │   9.29 % │   6.43 % │   8.57 % │   9.29 % │   6.43 % │     8.04 % │\n",
      "├─────────────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼──────────┼────────────┤\n",
      "│             ID3 │   20.0 % │  17.86 % │  16.43 % │  18.57 % │   15.0 % │  16.43 % │   15.0 % │  18.57 % │    17.23 % │\n",
      "╘═════════════════╧══════════╧══════════╧══════════╧══════════╧══════════╧══════════╧══════════╧══════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "errors_LR_total = 0\n",
    "errors_KNN_total = 0\n",
    "errors_NB_total = 0\n",
    "errors_ID3_total = 0\n",
    "\n",
    "total_iters = 8\n",
    "\n",
    "tablaIteraciones = []\n",
    "tablaErroresIteraciones = []\n",
    "\n",
    "tablaIteraciones.append([''])\n",
    "tablaErroresIteraciones.append([''])\n",
    "for i in range(total_iters):\n",
    "    tablaIteraciones[0].append('Iter ' + str(i + 1))\n",
    "\n",
    "for i in range(total_iters):\n",
    "    tablaErroresIteraciones[0].append('Iter ' + str(i + 1))\n",
    "tablaErroresIteraciones[0].append('Promedio')\n",
    "\n",
    "\n",
    "for iter in range(total_iters):\n",
    "    print('Realizando iteración ',iter +1, ':')\n",
    "    #######################################################################################\n",
    "    #######################       Regresión logística        ##############################\n",
    "    #######################################################################################\n",
    "    \n",
    "    # Se divide el conjunto de datos\n",
    "    numeric_validation_set, numeric_training_set = utils.split_20_80(numeric_data)\n",
    "\n",
    "\n",
    "    use_standarization = False\n",
    "    # Normalizamos el training set utilizando la tecnica min-max\n",
    "    training_set_scaled, scalation_parameters = utils.scale(copy.deepcopy(numeric_training_set), \n",
    "                                                            numeric_attributes,use_standarization)\n",
    "\n",
    "\n",
    "    # Normalizamos el validation set utilizando la tecnica min-max y\n",
    "    # los valores que usamos para normalizar el training\n",
    "    validation_set_scaled = []\n",
    "    for instance in numeric_validation_set:\n",
    "        scaled_instance = utils.scale_instance(copy.deepcopy(instance), scalation_parameters, use_standarization)\n",
    "        validation_set_scaled.append(scaled_instance)\n",
    "\n",
    "\n",
    "    # Se agrega 1 al inicio de los datos para\n",
    "    # que el producto escalar tenga termino sesgo\n",
    "    LR_numeric_attributes = copy.deepcopy(numeric_attributes)\n",
    "    LR_numeric_attributes.insert(0,'sesgo')\n",
    "    LR_training_set_scaled = copy.deepcopy(training_set_scaled)\n",
    "    LR_validation_set_scaled = copy.deepcopy(validation_set_scaled)\n",
    "    utils.insert_sesgo_one(LR_training_set_scaled)\n",
    "    utils.insert_sesgo_one(LR_validation_set_scaled)\n",
    "\n",
    "    # Weights para LR\n",
    "    weight = []\n",
    "    for i in range(len(LR_numeric_attributes)):\n",
    "        weight += [bestChoose[0]]\n",
    "\n",
    "    # Constante alpha de LR\n",
    "    alpha = bestChoose[1]\n",
    "\n",
    "    # Costo anterior\n",
    "    cost = float('inf')\n",
    "\n",
    "    # La condición de parado son 15 iteraciones o una \n",
    "    # diferencia de costos menor a 0.0001\n",
    "    for i in range(15):\n",
    "        newCost = utils.costFunction(weight, LR_training_set_scaled, LR_numeric_attributes, target_attr)\n",
    "        dif =abs(cost - newCost)\n",
    "        \n",
    "        if (iter == 0):\n",
    "            row = 'Aj. ' + repr(i)\n",
    "            tablaIteraciones.append([row])\n",
    "\n",
    "        tablaIteraciones[i+1].append(newCost)\n",
    "        \n",
    "        if (abs(cost - newCost) < 0.0001):\n",
    "            break\n",
    "        cost = newCost\n",
    "        weight = utils.descentByGradient(weight, LR_training_set_scaled, alpha, LR_numeric_attributes, target_attr)\n",
    "    \n",
    "    \n",
    "    # LR holdout validation\n",
    "    errors_LR = utils.LR_holdout_validation(LR_validation_set_scaled, target_attr, weight, LR_numeric_attributes)\n",
    "    errors_LR_total += errors_LR\n",
    "\n",
    "\n",
    "    #######################################################################################\n",
    "    #######################               KNN                ##############################\n",
    "    #######################################################################################\n",
    "    \n",
    "    # KNN holdout validation con k = 3 y usando pesos\n",
    "    errors_KNN = utils.KNN_holdout_validation(copy.deepcopy(training_set_scaled), \n",
    "                                              copy.deepcopy(validation_set_scaled), target_attr, \n",
    "                                              numeric_attributes, 3, True)\n",
    "    errors_KNN_total += errors_KNN[1]\n",
    "\n",
    "    \n",
    "    #######################################################################################\n",
    "    #######################         Naive Bayes              ##############################\n",
    "    #######################################################################################\n",
    "    \n",
    "    nb_classifier = NaiveBayes(copy.deepcopy(training_set_scaled), numeric_attributes, target_attr)\n",
    "    errors_NB = nb_classifier.holdout_validation(copy.deepcopy(validation_set_scaled), target_attr)\n",
    "    errors_NB_total += errors_NB[1]\n",
    "\n",
    "    \n",
    "    #######################################################################################\n",
    "    #######################            ID3                   ##############################\n",
    "    #######################################################################################\n",
    "    \n",
    "    tree = id3.ID3_algorithm(training_set_scaled, numeric_attributes, target_attr, True, False)\n",
    "    errors_ID3 = id3.validation(tree, validation_set_scaled, target_attr)\n",
    "    errors_ID3_total += errors_ID3\n",
    "\n",
    "    if (iter == 0):\n",
    "        validation_len = len(validation_set_scaled)\n",
    "        tablaErroresIteraciones.append(['Regr. logística'])\n",
    "        tablaErroresIteraciones.append(['KNN'])\n",
    "        tablaErroresIteraciones.append(['Naive Bayes'])\n",
    "        tablaErroresIteraciones.append(['ID3'])\n",
    "\n",
    "    tablaErroresIteraciones[1].append(str(round(errors_LR * 100 / validation_len, 2)) + ' %')\n",
    "    tablaErroresIteraciones[2].append(str(round(errors_KNN[1] * 100 / validation_len, 2)) + ' %')\n",
    "    tablaErroresIteraciones[3].append(str(round(errors_NB[1] * 100 / validation_len, 2)) + ' %')\n",
    "    tablaErroresIteraciones[4].append(str(round(errors_ID3 * 100 / validation_len, 2)) + ' %')\n",
    "\n",
    "print()\n",
    "print('El tamaño del conjunto de validación es:', validation_len)\n",
    "print()\n",
    "tablaErroresIteraciones[1].append(str(round(errors_LR_total/total_iters * 100 / validation_len, 2)) + ' %')\n",
    "tablaErroresIteraciones[2].append(str(round(errors_KNN_total/total_iters * 100 / validation_len, 2)) + ' %')\n",
    "tablaErroresIteraciones[3].append(str(round(errors_NB_total/total_iters * 100 / validation_len, 2)) + ' %')\n",
    "tablaErroresIteraciones[4].append(str(round(errors_ID3_total/total_iters * 100 / validation_len, 2)) + ' %')\n",
    "\n",
    "print()\n",
    "print('La siguiente tabla muestra el valor retornado por la función de costo')\n",
    "print('en cada uno de los ajustes (filas), para cada iteración (columnas)')\n",
    "print()\n",
    "print(tabulate(tablaIteraciones, headers='firstrow', tablefmt='fancy_grid', stralign='right', showindex=False))\n",
    "print()\n",
    "print('En la última tabla se observa el porcentaje de errores retornado por cada algoritmo (filas)')\n",
    "print('en cada iteración (columnas), como también el porcentaje del promedio de errrores')\n",
    "print('de todas las iteraciones (última columna)')\n",
    "print()\n",
    "print(tabulate(tablaErroresIteraciones, headers='firstrow', tablefmt='fancy_grid', stralign='right', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusiones\n",
    "Observando los resultados obtenidos en la búsqueda de los parámetros weight y alpha podemos concluir que no todos los parámetros elegidos funcionan correctamente para este ejercicio, una muestra de eso son los que tiene valor costo 'inf', esto representa que la función de costo divergió y alcanzo el valor máximo representable. Por otro lado, tenemos que las otras distintas combinaciones convergieron, algunas más rápidamente que otras. La combinación que logró el rendimiento más óptimo fue la de weight 0.1 y alpha 10. Posiblemente existan muchas otras combinaciones mejores para este ejercicio pero debido al costo que conlleva realizar el cálculo para cada una, se decidió elegir algunas representativas.\n",
    "Se puede concluir que un alpha 50 vuelve inestable la convergencia mientras que un alpha pequeño la vuelve muy lenta.\n",
    "\n",
    "\n",
    "Respecto a los resultados obtenidos por los distintos algoritmos, se aprecia la gran diferencia que hay entre regresión logística (LR) y los otros tres. El nivel de acierto de LR es menor de un uno porciento, lo que hace apenas se registren errores de clasificación. \n",
    "\n",
    "Por el otro lado, LR posee una gran complejidad, debe realizar muchísimos cálculos matemáticos para poder ajustar los valores del vector weight, esto genera que necesite un mucho mayor tiempo de ejecución comparado a los otros algoritmos contrastados.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
